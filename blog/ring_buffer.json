{
  "title": "AMD GPU Ring Buffer Architecture",
  "date": "January 2025",
  "category": "GPU Architecture",
  "description": "Deep dive into AMD's GPU driver ring buffer mechanism, exploring the sophisticated communication system between CPU and GPU in the Linux kernel AMDGPU driver with real code examples.",
  "readTime": "25 min read",
  "content": "# AMD GPU Ring Buffer Architecture\n\n## Background\n\nIn modern GPU architectures, efficient communication between CPU and GPU is crucial for performance. AMD's GPU driver implements a sophisticated **Ring Buffer** mechanism that enables asynchronous command submission and execution. This article explores how the current AMD GPU Linux kernel driver (AMDGPU) implements ring buffers based on the latest code.\n\n## GPU Communication Models\n\nThere are fundamentally two approaches to control hardware:\n\n**PUSH Model**: The CPU directly controls the GPU by reading/writing hardware registers\n- **Pros**: Simple, deterministic, immediate feedback\n- **Cons**: Synchronous, consumes CPU time, unsuitable for high-throughput workloads\n\n**PULL Model**: The CPU writes commands into buffers, GPU fetches and executes them\n- **Pros**: Asynchronous, parallel execution, efficient for GPU workloads\n- **Cons**: More complex, requires synchronization mechanisms\n\nAMD GPUs use the **PULL model** through ring buffers for optimal performance.\n\n## What is the AMD GPU Ring Buffer?\n\nThink of it as a **producer-consumer circular queue**:\n\n```\nCPU (Producer) → [Ring Buffer] ← GPU (Consumer)\n                      ↓\n               Shared Memory Region\n```\n\n![AMD GPU Ring Buffer Architecture](../assets/images/ring_buffer.png)\n\nThe ring buffer consists of:\n- **Buffer Base**: Physical memory address accessible by both CPU and GPU\n- **wptr (Write Pointer)**: Managed by CPU, points to next write location\n- **rptr (Read Pointer)**: Updated by GPU, points to next read location\n- **Buffer Size**: Predefined circular buffer size\n\n### Buffer Status Logic\n\n```c\n// Buffer status check logic\nif (rptr == wptr) {\n    // Buffer is empty, no commands to process\n    return RING_EMPTY;\n} else {\n    // Buffer has commands, GPU will execute them\n    return RING_HAS_COMMANDS;\n}\n```\n\n## Current Ring Buffer Data Structures\n\n### Core Ring Structure\n\n```c\n//drivers/gpu/drm/amd/amdgpu/amdgpu_ring.h\nstruct amdgpu_ring {\n    struct amdgpu_device        *adev;\n    const struct amdgpu_ring_funcs *funcs;\n    \n    /* Ring buffer memory */\n    struct amdgpu_bo           *ring_obj;\n    uint32_t                   *ring;           /* CPU mapping */\n    uint64_t                   gpu_addr;        /* GPU address */\n    unsigned                   ring_size;       /* Size in bytes */\n    \n    /* Pointer management */\n    u64                        wptr;            /* Write pointer */\n    u64                        wptr_old;        /* Previous wptr */\n    unsigned                   rptr_offs;       /* rptr offset */\n    u64                        rptr_gpu_addr;   /* rptr GPU address */\n    u32                        *rptr_cpu_addr;  /* rptr CPU mapping */\n    \n    /* Buffer masks and limits */\n    uint64_t                   ptr_mask;        /* 64-bit pointer mask */\n    uint32_t                   buf_mask;        /* Buffer wrap mask */\n    unsigned                   max_dw;          /* Max DWords per submission */\n    int                        count_dw;        /* Current allocation count */\n    \n    /* Doorbell mechanism */\n    u32                        doorbell_index;  /* Doorbell register index */\n    bool                       use_doorbell;    /* Use doorbell vs register */\n    u32                        *wptr_cpu_addr;  /* wptr CPU mapping */\n    u64                        wptr_gpu_addr;   /* wptr GPU address */\n    \n    /* Ring identification */\n    char                       name[16];        /* Ring name */\n    enum amdgpu_ring_type      type;           /* Ring type */\n    u32                        idx;            /* Ring index */\n};\n```\n\n## Hardware Ring Queues in Modern AMD GPUs\n\nCurrent AMD GPUs support multiple ring types with varying quantities:\n\n| Engine | Ring Count | Purpose | Notes |\n|--------|------------|---------|-------|\n| **GFX** | 1-2 | 3D Graphics rendering | High-performance 3D, multiple priorities |\n| **Compute** | 8-32 | Mathematical computation | Matrix operations, ML workloads |\n| **SDMA** | 1-4 | System DMA operations | Memory copy/fill/move operations |\n| **VCN_DEC** | 1-2 | Video decode | Hardware video decoding (VCN replaces UVD) |\n| **VCN_ENC** | 1-2 | Video encode | Hardware video encoding (VCN replaces VCE) |\n| **VCN_JPEG** | 1-2 | JPEG decode/encode | Dedicated JPEG processing |\n| **VPE** | 1-2 | Video Processing Engine | Video post-processing |\n| **KIQ** | 1 | Kernel Interface Queue | Special control ring |\n| **MES** | 1+ | Micro Engine Scheduler | Hardware scheduling |\n| **UVD** | Legacy | Legacy Video Decode | Replaced by VCN_DEC in modern GPUs |\n| **VCE** | Legacy | Legacy Video Encode | Replaced by VCN_ENC in modern GPUs |\n\n### Engine Evolution Notes\n\n**Important**: AMD has transitioned from older **UVD (Universal Video Decoder)** and **VCE (Video Compression Engine)** to the modern **VCN (Video Core Next)** architecture starting with Vega and continuing through RDNA/CDNA generations. VCN provides:\n\n- **Better Power Efficiency**: Improved power management for video workloads\n- **Higher Performance**: Enhanced throughput for 4K/8K video processing  \n- **Modern Codecs**: Support for AV1, H.265/HEVC, VP9 hardware acceleration\n- **Unified Architecture**: Single video engine handling both encode/decode\n\n### Maximum Ring Limits\n\n```c\n#define AMDGPU_MAX_RINGS        149\n#define AMDGPU_MAX_HWIP_RINGS   64\n#define AMDGPU_MAX_GFX_RINGS    2\n#define AMDGPU_MAX_COMPUTE_RINGS 8\n```\n\n## Command Packet Formats\n\n### PM4 (Packet Manager 4) - The Command Processor Language\n\n**PM4** is AMD's **native command format** used by the Command Processor (CP) hardware to control GPU operations. PM4 has been the foundation of AMD GPU command submission since the R600 generation (2006) and continues to be the primary interface for all AMD RDNA and CDNA architectures.\n\n#### **What is PM4?**\n\nPM4 (Packet Manager 4) serves as the **instruction set for the GPU's Command Processor**, which acts as a dedicated microcontroller responsible for:\n\n- **Command Parsing**: Reading and interpreting commands from ring buffers\n- **State Management**: Tracking GPU pipeline state and context switching\n- **Resource Management**: Managing memory, synchronization, and resource allocation\n- **Hardware Control**: Configuring graphics and compute engines\n\n#### **PM4 Packet Structure and DWords**\n\nAMD GPUs use a **packet-based command format** where commands consist of **DWords (32-bit words)**. All PM4 commands are structured as sequences of DWords for hardware optimization:\n\n**Why DWords (32-bit)?**\n- **Hardware Alignment**: AMD GPU command processors are optimized for 32-bit aligned memory access\n- **Efficient Addressing**: PCIe and GPU memory controllers work optimally with 32-bit boundaries  \n- **Register Interface**: GPU registers are inherently 32-bit, matching the command word size\n- **Atomic Operations**: 32-bit writes are guaranteed atomic on modern CPU and GPU architectures\n\n**Real PM4 Command Example** (from `drivers/gpu/drm/amd/amdgpu/gfx_v11_0.c`):\n```c\n// PACKET3_SET_RESOURCES command - 8 DWords total\namdgpu_ring_write(kiq_ring, PACKET3(PACKET3_SET_RESOURCES, 6));  // Header: 1 DWord\namdgpu_ring_write(kiq_ring, PACKET3_SET_RESOURCES_VMID_MASK(0)); // Param 1: 1 DWord  \namdgpu_ring_write(kiq_ring, lower_32_bits(queue_mask));          // Param 2: 1 DWord\namdgpu_ring_write(kiq_ring, upper_32_bits(queue_mask));          // Param 3: 1 DWord\namdgpu_ring_write(kiq_ring, lower_32_bits(shader_mc_addr));      // Param 4: 1 DWord\namdgpu_ring_write(kiq_ring, upper_32_bits(shader_mc_addr));      // Param 5: 1 DWord\namdgpu_ring_write(kiq_ring, 0);                                  // Param 6: 1 DWord  \namdgpu_ring_write(kiq_ring, 0);                                  // Param 7: 1 DWord\n// Total: 8 DWords = 32 bytes\n```\n\n**Type 0 (PACKET0)**: Direct register writes (from `drivers/gpu/drm/amd/amdgpu/soc15d.h`)\n```c\n#define PACKET0(reg, n) ((PACKET_TYPE0 << 30) | ((reg) & 0xFFFF) | ((n) & 0x3FFF) << 16)\n\n// Header format: [31:30]=0, [29:16]=COUNT, [15:0]=REG_OFFSET\n// Writes (n+1) consecutive DWords to registers starting from REG_OFFSET\n// Example: PACKET0(mmUVD_SEMA_WAIT_FAULT_TIMEOUT_CNTL, 0) - writes 1 DWord to register\n// Used in: drivers/gpu/drm/amd/amdgpu/uvd_v4_2.c, amdgpu_jpeg.c\n```\n\n**Type 2 (PACKET2)**: Filler/Padding packets  \n```c\n#define CP_PACKET2 0x80000000\n// Header format: [31:30]=2, [29:0]=FILLER_DATA  \n// Used for memory alignment and no-operation padding\n// Essential for maintaining proper command alignment\n```\n\n**Type 3 (PACKET3)**: Primary command format (modern standard)\n```c\n// Header format from drivers/gpu/drm/amd/amdgpu/soc15d.h\n#define PACKET3(op, n) ((PACKET_TYPE3 << 30) | (((op) & 0xFF) << 8) | ((n) & 0x3FFF))\n\n// Field breakdown:\n// [31:30] = 3           (Packet Type)\n// [29:16] = Reserved    (Future extensions)  \n// [15:8]  = IT_OPCODE   (Instruction opcode - see below)\n// [13:0]  = COUNT       (Payload length - 1)\n```\n\n#### **PM4 Architecture Benefits**\n\n1. **Hardware Acceleration**: Commands executed directly by dedicated CP hardware\n2. **Asynchronous Execution**: CPU can queue commands without waiting for completion\n3. **Context Switching**: Efficient switching between graphics and compute workloads\n4. **Memory Coherency**: Built-in cache management and memory synchronization\n5. **Error Handling**: Hardware-level error detection and recovery\n\n### Common PKT3 Opcodes - The Command Vocabulary\n\nPM4 PKT3 opcodes represent **specific hardware operations** that the Command Processor can execute. Here are the key opcodes used in modern AMD GPU drivers:\n\n```c\n//drivers/gpu/drm/amd/amdgpu/soc15d.h (and similar in vid.h, nvd.h, cikd.h)\n\n// === Basic Operations ===\n#define PACKET3_NOP                    0x10  // No operation (padding/sync)\n#define PACKET3_SET_BASE              0x11  // Set base address for operations  \n#define PACKET3_CLEAR_STATE           0x12  // Clear GPU pipeline state\n#define PACKET3_INDEX_BUFFER_SIZE     0x13  // Set index buffer size\n\n// === Compute Operations ===\n#define PACKET3_DISPATCH_DIRECT       0x15  // Launch compute shader directly\n#define PACKET3_DISPATCH_INDIRECT     0x16  // Launch compute via indirect buffer\n\n// === Memory Operations ===\n#define PACKET3_ATOMIC_MEM            0x1E  // Atomic memory operations\n#define PACKET3_WRITE_DATA            0x37  // Write data to memory/registers\n#define PACKET3_COPY_DATA             0x40  // Copy data between locations\n\n// === Control Flow ===\n#define PACKET3_COND_EXEC             0x22  // Conditional execution\n#define PACKET3_PRED_EXEC             0x23  // Predicated execution\n#define PACKET3_INDIRECT_BUFFER       0x3F  // Execute indirect command buffer\n\n// === Synchronization ===\n#define PACKET3_WAIT_REG_MEM          0x3C  // Wait for register/memory condition\n#define PACKET3_MEM_SEMAPHORE         0x39  // Memory semaphore operations\n#define PACKET3_RELEASE_MEM           0x49  // Memory release with fence\n\n// === Graphics Pipeline ===\n#define PACKET3_DRAW_INDEX_AUTO       0x2D  // Auto-generate draw indices\n#define PACKET3_DRAW_INDIRECT         0x24  // Indirect draw commands\n#define PACKET3_SET_UCONFIG_REG       0x79  // Set user config register\n\n// === Advanced Features (RDNA) ===\n#define PACKET3_EVENT_WRITE           0x46  // Write GPU events\n#define PACKET3_EVENT_WRITE_EOP       0x47  // End-of-pipe event with cache flush\n```\n\n#### **PKT3 Command Categories**\n\n**1. State Management**\n- `PACKET3_CLEAR_STATE`: Resets GPU pipeline to known state\n- `PACKET3_SET_BASE`: Establishes base addresses for shaders/resources\n- `PACKET3_SET_UCONFIG_REG`: Configures user-accessible registers\n\n**2. Compute Dispatch** \n- `PACKET3_DISPATCH_DIRECT`: Direct compute shader launch with threadgroup counts\n- `PACKET3_DISPATCH_INDIRECT`: Compute launch with GPU-generated parameters\n\n**3. Memory Management**\n- `PACKET3_WRITE_DATA`: High-performance data writes to GPU memory\n- `PACKET3_ATOMIC_MEM`: Hardware-accelerated atomic operations\n- `PACKET3_COPY_DATA`: Efficient memory-to-memory transfers\n\n**4. Synchronization & Fencing**\n- `PACKET3_RELEASE_MEM`: The **primary fencing mechanism** (see fence section)\n- `PACKET3_WAIT_REG_MEM`: Wait for memory/register conditions\n- `PACKET3_MEM_SEMAPHORE`: GPU semaphore operations\n\n#### **AMD GPU Architecture Evolution**\n\nPM4 has evolved alongside AMD's GPU architectures:\n\n| **Architecture** | **Era** | **PM4 Features** | **Documentation** |\n|------------------|---------|------------------|-------------------|\n| **R600/R700** | 2006-2009 | Basic PM4, PKT0/PKT3 | Historical |\n| **GCN 1-3** | 2012-2015 | Enhanced compute, async | [GCN3 ISA](https://www.amd.com/system/files/TechDocs/gcn3-instruction-set-architecture.pdf) |\n| **Vega** | 2017 | Advanced features, HBM | [Vega ISA](https://www.amd.com/system/files/TechDocs/vega-shader-instruction-set-architecture.pdf) |\n| **RDNA 1** | 2019 | Wave32, new cache hierarchy | [RDNA ISA](https://www.amd.com/system/files/TechDocs/rdna-shader-instruction-set-architecture.pdf) |\n| **RDNA 2** | 2020 | Hardware RT, VRS, mesh shaders | [RDNA2 ISA](https://www.amd.com/system/files/TechDocs/rdna2-shader-instruction-set-architecture.pdf) |\n| **RDNA 3** | 2022 | Chiplet design, enhanced RT | [RDNA3 ISA](https://www.amd.com/system/files/TechDocs/rdna3-shader-instruction-set-architecture-feb-2023_0.pdf) |\n| **RDNA 4** | 2025 | Matrix cores, AI acceleration | [RDNA4 ISA](https://www.amd.com/content/dam/amd/en/documents/radeon-tech-docs/instruction-set-architectures/rdna4-instruction-set-architecture.pdf) |\n\n#### **PM4 vs Other Command Formats**\n\n**PM4 vs DirectX/Vulkan APIs**: \nPM4 is the **low-level hardware interface** that high-level graphics APIs ultimately translate to:\n- **DirectX 12/11**: The DirectX runtime translates D3D commands into PM4 packets for AMD hardware\n- **Vulkan**: Vulkan drivers convert VkCmd* functions into PM4 command streams \n- **OpenGL**: Legacy OpenGL state changes are batched and converted to PM4 packets\n- **Translation Layer**: User-mode drivers (UMD) perform this translation, kernel driver only sees PM4\n\n**PM4 vs AQL (Architected Queuing Language)**:\nAQL and PM4 serve different purposes in the AMD ecosystem:\n- **AQL Usage**: HSA/ROCm compute workloads use AQL packets for **compute-only dispatch**\n- **AQL Structure**: Fixed 64-byte packet format optimized for HSA compute kernels\n- **PM4 Usage**: **Graphics, video, and general GPU control** still use PM4 packets\n- **Coexistence**: Modern AMD GPUs support both - AQL queues for compute, PM4 for everything else\n- **Driver Implementation**: `drivers/gpu/drm/amd/amdgpu/amdgpu_amdkfd*.c` handles AQL, PM4 handled by main driver\n\n**PM4 vs NVIDIA's Command Streams**:\nFundamental architectural differences between AMD and NVIDIA approaches:\n- **AMD PM4**: **Publicly documented** packet format with open-source driver support\n- **NVIDIA Methods**: Proprietary \"pushbuffer methods\" with limited public documentation\n- **Flexibility**: AMD's packet-based approach vs NVIDIA's method-based approach  \n- **Open Source**: AMD provides complete PM4 specifications, NVIDIA methods are largely undocumented\n- **Community**: PM4 enables community GPU driver development (Mesa, etc.)\n\n**PM4 vs Intel GPU Command Streams**:\nIntel uses a different architecture for their integrated GPUs:\n- **Intel Ring Buffers**: Similar concept but different packet formats and opcodes\n- **Intel MI_* Commands**: Intel's equivalent to AMD's PACKET3 opcodes\n- **Architecture Focus**: Intel optimized for integrated graphics, AMD for discrete high-performance\n- **Documentation**: Both provide open documentation, but different command set philosophies\n\n---\n*For complete PM4 documentation, see [AMD GPU Architecture Programming Documentation](https://gpuopen.com/amd-gpu-architecture-programming-documentation/) on GPUOpen.*\n\n## Ring Buffer Operations\n\n### 1. Ring Allocation\n\n```c\n//drivers/gpu/drm/amd/amdgpu/amdgpu_ring.c\nint amdgpu_ring_alloc(struct amdgpu_ring *ring, unsigned int ndw)\n{\n    // Align to hardware requirements\n    ndw = (ndw + ring->funcs->align_mask) & ~ring->funcs->align_mask;\n    \n    // Check maximum submission limit\n    if (WARN_ON_ONCE(ndw > ring->max_dw))\n        return -ENOMEM;\n        \n    ring->count_dw = ndw;\n    ring->wptr_old = ring->wptr;  // Save old position\n    \n    if (ring->funcs->begin_use)\n        ring->funcs->begin_use(ring);  // Power gating control\n        \n    return 0;\n}\n```\n\n### 2. Writing Commands\n\n```c\n//drivers/gpu/drm/amd/amdgpu/amdgpu_ring.h\n// Write single DWORD to ring\nstatic inline void amdgpu_ring_write(struct amdgpu_ring *ring, uint32_t v)\n{\n    ring->ring[ring->wptr & ring->buf_mask] = v;\n    ring->wptr++;\n    ring->count_dw--;\n}\n```\n\n**Key Implementation Details:**\n- **Power-of-2 Sizing**: Ring buffers must be power-of-2 sized for efficient wrap-around\n- **Buffer Mask**: `buf_mask = (ring_size / 4) - 1` for fast modulo operations\n- **Atomic Operations**: wptr updates use memory barriers to ensure visibility\n\n### 3. Ring Commit\n\n```c\n//drivers/gpu/drm/amd/amdgpu/amdgpu_ring.h\nvoid amdgpu_ring_commit(struct amdgpu_ring *ring)\n{\n    // Add padding for alignment\n    uint32_t count = ring->funcs->align_mask + 1 - \n                     (ring->wptr & ring->funcs->align_mask);\n    count &= ring->funcs->align_mask;\n    \n    if (count != 0)\n        ring->funcs->insert_nop(ring, count);  // Pad with NOPs\n        \n    mb();  // Memory barrier\n    amdgpu_ring_set_wptr(ring);  // Update hardware\n    \n    if (ring->funcs->end_use)\n        ring->funcs->end_use(ring);  // Power gating control\n}\n```\n\n## Doorbell Mechanism - Modern Implementation\n\nThe doorbell mechanism has evolved significantly. Modern AMD GPUs allocate dedicated PCIe BAR regions for doorbell registers.\n\n### Doorbell vs Register Write\n\n```c\n////drivers/gpu/drm/amd/amdgpu/gfx_v11_0.c\nstatic void gfx_v11_0_ring_set_wptr_gfx(struct amdgpu_ring *ring)\n{\n    struct amdgpu_device *adev = ring->adev;\n    \n    if (ring->use_doorbell) {\n        // Update shared memory location\n        atomic64_set((atomic64_t *)ring->wptr_cpu_addr, ring->wptr);\n        \n        // Ring doorbell to notify GPU\n        WDOORBELL64(ring->doorbell_index, ring->wptr);\n    } else {\n        // Fallback: direct register write\n        WREG32_SOC15(GC, 0, regCP_RB0_WPTR, lower_32_bits(ring->wptr));\n        WREG32_SOC15(GC, 0, regCP_RB0_WPTR_HI, upper_32_bits(ring->wptr));\n    }\n}\n```\n\n### Why Doorbells?\n\n1. **User Mode Queues**: Enables direct user space submissions\n2. **Reduced Latency**: No kernel register access required  \n3. **Scalability**: Each ring gets its own doorbell\n4. **Power Efficiency**: GPU can sleep until doorbell rings\n\n### Doorbell Implementation Details\n\nModern AMD GPUs allocate **dedicated PCIe BAR space** for doorbell registers:\n\n- **BAR Region**: Usually BAR2 or BAR5 (varies by GPU generation)\n- **Size**: Typically 8MB region divided into 8-byte doorbell slots\n- **Addressing**: Each ring gets unique doorbell index\n- **Atomic Operations**: Hardware ensures atomic doorbell writes\n\n```c\n// Doorbell index assignment example\nring->doorbell_index = (adev->doorbell_index.mec_ring0 + ring_id) << 1;\n\n// Multiple doorbell writes for complex rings (VPE example)\nif (ring->use_doorbell) {\n    WDOORBELL64(ring->doorbell_index, ring->wptr << 2);\n    if (ring->pipe == 1)  // Secondary pipe\n        WDOORBELL64(ring->doorbell_index + 4, ring->wptr << 2);\n}\n```\n\n## Ring Test Example: Modern Implementation\n\nHere's how the current driver tests ring functionality:\n\n```c\n////drivers/gpu/drm/amd/amdgpu/gfx_v11_0.c\nstatic int gfx_v11_0_ring_test_ring(struct amdgpu_ring *ring)\n{\n    struct amdgpu_device *adev = ring->adev;\n    uint32_t scratch = SOC15_REG_OFFSET(GC, 0, regSCRATCH_REG0);\n    uint32_t tmp = 0;\n    unsigned i;\n    int r;\n\n    // 1. Initialize scratch register with known value\n    WREG32(scratch, 0xCAFEDEAD);\n    \n    // 2. Allocate space in ring buffer (5 DWORDs)\n    r = amdgpu_ring_alloc(ring, 5);\n    if (r) {\n        DRM_ERROR(\"amdgpu: cp failed to lock ring %d (%d).\\n\", \n                  ring->idx, r);\n        return r;\n    }\n\n    // 3. Write command to change scratch register\n    if (ring->funcs->type == AMDGPU_RING_TYPE_KIQ) {\n        // KIQ uses special write register command\n        gfx_v11_0_ring_emit_wreg(ring, scratch, 0xDEADBEEF);\n    } else {\n        // Standard rings use PACKET3_SET_UCONFIG_REG\n        amdgpu_ring_write(ring, PACKET3(PACKET3_SET_UCONFIG_REG, 1));\n        amdgpu_ring_write(ring, scratch - PACKET3_SET_UCONFIG_REG_START);\n        amdgpu_ring_write(ring, 0xDEADBEEF);  // New value\n    }\n    \n    // 4. Commit commands to GPU\n    amdgpu_ring_commit(ring);\n\n    // 5. Poll for completion\n    for (i = 0; i < adev->usec_timeout; i++) {\n        tmp = RREG32(scratch);\n        if (tmp == 0xDEADBEEF)  // Success!\n            break;\n        if (amdgpu_emu_mode == 1)\n            msleep(1);\n        else\n            udelay(1);\n    }\n\n    if (i >= adev->usec_timeout)\n        r = -ETIMEDOUT;\n        \n    return r;\n}\n```\n\n## Indirect Buffers (IB) - Advanced Command Submission\n\nFor complex workloads, direct ring submission can be inefficient. **Indirect Buffers** provide a solution:\n\n### IB Structure\n```c\n////drivers/gpu/drm/amd/amdgpu/amdgpu_ring.h\nstruct amdgpu_ib {\n    struct drm_suballoc    *sa_bo;      /* Suballocated buffer object */\n    uint32_t               length_dw;    /* Length in DWORDs */\n    uint64_t               gpu_addr;     /* GPU address */\n    uint32_t               *ptr;         /* CPU mapping */\n    uint32_t               flags;        /* IB flags */\n};\n```\n\n### IB Submission Process\n\n1. **Allocate IB**: Get buffer from IB pool\n2. **Fill Commands**: Write commands to IB buffer  \n3. **Submit to Ring**: Use `PACKET3_INDIRECT_BUFFER` to reference IB\n4. **GPU Execution**: GPU jumps to IB, executes, returns to ring\n\n### IB Pool Types\n```c\nenum amdgpu_ib_pool_type {\n    AMDGPU_IB_POOL_DELAYED,    /* Normal submissions */\n    AMDGPU_IB_POOL_IMMEDIATE,  /* High priority submissions */\n    AMDGPU_IB_POOL_DIRECT,     /* Direct ring access */\n    AMDGPU_IB_POOL_MAX\n};\n```\n\n## Fence Mechanism - Synchronization\n\n### Modern Fence Structure\n```c\nstruct amdgpu_fence {\n    struct dma_fence    base;           /* Linux DMA fence */\n    struct amdgpu_ring *ring;           /* Associated ring */\n    ktime_t            start_timestamp; /* Start time */\n    u64                wptr;            /* Write pointer when emitted */\n    u64                context;         /* Fence context ID */\n    uint32_t           seq;             /* Sequence number */\n};\n```\n\n### Fence Emission - Real Driver Implementation\n\n```c\n//drivers/gpu/drm/amd/amdgpu/gfx_v11_0.c\nstatic void gfx_v11_0_ring_emit_fence(struct amdgpu_ring *ring, u64 addr,\n                                     u64 seq, unsigned flags)\n{\n    bool write64bit = flags & AMDGPU_FENCE_FLAG_64BIT;\n    bool int_sel = flags & AMDGPU_FENCE_FLAG_INT;\n\n    /* RELEASE_MEM - flush caches, send int */\n    amdgpu_ring_write(ring, PACKET3(PACKET3_RELEASE_MEM, 6));\n    amdgpu_ring_write(ring, (PACKET3_RELEASE_MEM_GCR_SEQ |\n                             PACKET3_RELEASE_MEM_GCR_GL2_WB |\n                             PACKET3_RELEASE_MEM_GCR_GLM_INV | /* must be set with GLM_WB */\n                             PACKET3_RELEASE_MEM_GCR_GLM_WB |\n                             PACKET3_RELEASE_MEM_CACHE_POLICY(3) |\n                             PACKET3_RELEASE_MEM_EVENT_TYPE(CACHE_FLUSH_AND_INV_TS_EVENT) |\n                             PACKET3_RELEASE_MEM_EVENT_INDEX(5)));\n    amdgpu_ring_write(ring, (PACKET3_RELEASE_MEM_DATA_SEL(write64bit ? 2 : 1) |\n                             PACKET3_RELEASE_MEM_INT_SEL(int_sel ? 2 : 0)));\n\n    /* Address must be Qword aligned for 64bit, Dword aligned for 32bit */\n    if (write64bit)\n        BUG_ON(addr & 0x7);  // 8-byte alignment check\n    else\n        BUG_ON(addr & 0x3);  // 4-byte alignment check\n        \n    amdgpu_ring_write(ring, lower_32_bits(addr));    // Address low\n    amdgpu_ring_write(ring, upper_32_bits(addr));    // Address high\n    amdgpu_ring_write(ring, lower_32_bits(seq));     // Sequence low\n    amdgpu_ring_write(ring, upper_32_bits(seq));     // Sequence high\n    amdgpu_ring_write(ring, 0);                      // Reserved\n}\n```\n\n## Ring Buffer Memory Management\n\n### Buffer Allocation\n```c\n//drivers/gpu/drm/amd/amdgpu/amdgpu_ring.c\nint amdgpu_ring_init(struct amdgpu_device *adev, struct amdgpu_ring *ring,\n                    unsigned int max_dw, struct amdgpu_irq_src *irq_src,\n                    unsigned int irq_type, unsigned int hw_prio,\n                    atomic_t *sched_score)\n{\n    // Calculate buffer size (must be power of 2)\n    ring->ring_size = roundup_pow_of_two(max_dw * 4 * amdgpu_sched_hw_submission);\n    ring->buf_mask = (ring->ring_size / 4) - 1;\n    \n    // Allocate GPU buffer object\n    r = amdgpu_bo_create_kernel(adev, ring->ring_size + ring->funcs->extra_dw,\n                               PAGE_SIZE, AMDGPU_GEM_DOMAIN_GTT,\n                               &ring->ring_obj, &ring->gpu_addr,\n                               (void **)&ring->ring);\n    \n    // Setup pointer tracking\n    ring->wptr = 0;\n    ring->rptr_offs = adev->wb.gpu_addr + (ring->rptr_offs * 4);\n    ring->wptr_offs = adev->wb.gpu_addr + (ring->wptr_offs * 4);\n    \n    return 0;\n}\n```\n\n### Ring Buffer Sizing: Power-of-2 Architecture\n\nAMD GPU ring buffers **must be power-of-2 sized** for optimal hardware performance. This requirement is enforced in `amdgpu_ring_init()`:\n\n```c\n//drivers/gpu/drm/amd/amdgpu/amdgpu_ring.c\nring->ring_size = roundup_pow_of_two(max_dw * 4 * amdgpu_sched_hw_submission);\n```\n\n#### **Why Power-of-2 Sizing?**\n\n**Mathematical Efficiency**: Power-of-2 sizes enable **bitwise AND operations** instead of expensive modulo division:\n\n```c\n// With power-of-2 size (e.g., 32KB = 32768 = 0x8000)\n#define RING_SIZE_MASK (32768 - 1)  // 0x7FFF\n\n// Fast wraparound using bitwise AND (1 CPU cycle):\nnext_position = (current_position + increment) & RING_SIZE_MASK;\n\n// Instead of slow modulo operation (~20+ CPU cycles):\nnext_position = (current_position + increment) % 32768;\n```\n\n**Hardware Optimization Benefits**:\n1. **GPU Memory Controllers**: Work more efficiently with power-of-2 aligned buffers\n2. **Cache Line Alignment**: Better GPU cache utilization and reduced memory stalls\n3. **MMU Performance**: GPU page tables optimized for power-of-2 memory regions\n4. **DMA Efficiency**: System DMA engines prefer power-of-2 transfer sizes\n\n#### **Ring Size Calculation**\n\nThe ring buffer size formula from the driver:\n```c\nfinal_size = roundup_pow_of_two(max_dw * 4 * sched_hw_submission)\n```\n\n**Parameter Breakdown**:\n- **max_dw**: Maximum DWords (32-bit words) per command submission\n- **4**: Conversion factor (DWords to bytes: 4 bytes per DWord)  \n- **sched_hw_submission**: Number of jobs that can be queued simultaneously (typically 8)\n\n**Example Calculation**:\n```c\n// For a typical GFX ring:\nmax_dw = 1024              // 1024 DWords per command\nsched_hw_submission = 8    // 8 simultaneous submissions\nrequired_size = 1024 * 4 * 8 = 32,768 bytes (32KB)\nfinal_size = roundup_pow_of_two(32768) = 32768  // Already power-of-2\n```\n\n**Real-World Ring Sizes**:\n| **Ring Type** | **max_dw** | **Calculated Size** | **Typical Final Size** |\n|---------------|------------|---------------------|------------------------|\n| **GFX** | 1024 | 32KB | 32KB |\n| **Compute** | 1024 | 32KB | 32KB |\n| **SDMA** | 256 | 8KB | 8KB |\n| **VCN** | 512 | 16KB | 16KB |\n\nThe `buf_mask = (ring_size / 4) - 1` creates an efficient wraparound mask for DWord indexing.\n\n## Performance Considerations\n\n### Ring Buffer Sizing\n- **Too Small**: Frequent submissions, CPU overhead\n- **Too Large**: Memory waste, higher latency  \n- **Sweet Spot**: Balance based on workload patterns\n- **Power-of-2 Requirement**: Mandatory for efficient hardware addressing and wraparound operations\n\n### Modern Optimizations\n1. **Multiple Submission**: Batch multiple IBs per submission\n2. **Priority Scheduling**: Different rings for different priorities\n3. **Power Gating**: Dynamic power management \n4. **VMID Management**: Virtual memory context switching\n\n## Conclusion\n\nThe AMD GPU Ring Buffer architecture enables efficient and asynchronous communication between the CPU and GPU by:\n\n- **Using ring buffers for command passing**: Power-of-2 sized circular queues with efficient DWord-based addressing\n- **PM4 command format**: Hardware-optimized packet structure using 32-bit DWords for all GPU operations\n- **Packet diversity**: PACKET0 for register writes, PACKET3 for complex operations (no PACKET4 exists)\n- **Mathematical optimization**: Power-of-2 sizing enables bitwise operations instead of expensive modulo arithmetic\n- **Utilizing doorbells for event notification**: Dedicated PCIe BAR regions for low-latency notifications\n- **Implementing fences for synchronization**: Advanced cache-coherent synchronization with hardware interrupts\n- **Supporting indirect command buffers (IBs)**: Hierarchical command submission to reduce ring buffer usage\n- **Modern engine architecture**: Transition from legacy UVD/VCE to VCN (Video Core Next) for improved performance\n\n## Recent Updates and Corrections\n\n### Key Corrections Made to This Document (2025):\n\n1. **Engine Names Updated**: \n   - Corrected legacy \"UVD/VCE\" references to modern \"VCN_DEC/VCN_ENC\"\n   - Added missing engines: KIQ, MES, UMSCH_MM, CPER\n   \n2. **Accurate Ring Counts**: \n   - Fixed SDMA ring counts from unclear \"~10×(1–7)\" to accurate \"1-4\"\n   - Updated based on current driver implementations across GPU generations\n\n3. **Enhanced Implementation Details**:\n   - Added power-of-2 buffer sizing requirements\n   - Included buffer mask calculations and alignment constraints\n   - Enhanced fence implementation with cache flushing details\n\n4. **Modern Architecture References**:\n   - Updated for RDNA3/RDNA4 and CDNA3 architectures\n   - Added references to current AMD documentation and ISA guides\n\n*This document reflects the state of AMD GPU ring buffer implementation as of Linux kernel 6.x (2025) and AMDGPU driver development.*\n\n## Conclusion\n\nThe modern AMD GPU ring buffer architecture represents a sophisticated evolution from earlier designs:\n\n**Key Improvements:**\n- **Scalable Doorbell System**: Efficient notification mechanism\n- **Multiple Ring Types**: Specialized rings for different workloads  \n- **Advanced Fence Management**: Integration with Linux DMA fence framework\n- **Power Efficiency**: Dynamic power management integration\n- **Flexible IB System**: Efficient command buffer management\n\n**Benefits:**\n- **Asynchronous Execution**: CPU and GPU work in parallel\n- **High Throughput**: Multiple concurrent command streams\n- **Power Efficiency**: Hardware sleep/wake optimization\n- **Scalability**: Supports complex multi-engine workloads\n\nThe ring buffer mechanism remains the foundation of AMD GPU command submission, enabling the high-performance computing and graphics capabilities that modern applications demand.",
  "source_file": "blog/ring_buffer.md",
  "generated_at": "/home/amd/github/alibukharai.github.io"
}